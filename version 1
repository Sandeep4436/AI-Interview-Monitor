import { useRef, useState, useEffect } from 'react';
import { Button } from "@/components/ui/button";

export default function InterviewMonitor() {
  const videoRef = useRef(null);
  const [stream, setStream] = useState(null);
  const [isRecording, setIsRecording] = useState(false);
  
  useEffect(() => {
    async function getVideo() {
      try {
        const userStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        setStream(userStream);
        if (videoRef.current) {
          videoRef.current.srcObject = userStream;
        }
      } catch (err) {
        console.error("Error accessing webcam: ", err);
      }
    }
    getVideo();
  }, []);

  const startRecording = () => {
    setIsRecording(true);
    // TODO: Implement AI processing via backend
  };

  const stopRecording = () => {
    setIsRecording(false);
    // TODO: Stop AI processing and analyze results
  };

  return (
    <div className="flex flex-col items-center p-6 bg-gray-100 min-h-screen">
      <h1 className="text-2xl font-bold mb-4">AI Interview Monitor</h1>
      <video ref={videoRef} autoPlay playsInline className="rounded-lg border shadow-lg w-3/4" />
      <div className="mt-4">
        {isRecording ? (
          <Button onClick={stopRecording} className="bg-red-500 hover:bg-red-700">Stop Recording</Button>
        ) : (
          <Button onClick={startRecording} className="bg-green-500 hover:bg-green-700">Start Interview</Button>
        )}
      </div>
    </div>
  );
}

from fastapi import FastAPI, UploadFile, File
import cv2
import numpy as np
import asyncio
import tempfile
from pydub import AudioSegment
import mediapipe as mp
import deepspeech

app = FastAPI()

mp_face_mesh = mp.solutions.face_mesh.FaceMesh()
deepspeech_model = deepspeech.Model('deepspeech_model.pbmm')

@app.post("/api/start-recording")
def start_recording():
    return {"message": "Recording started"}

@app.post("/api/stop-recording")
async def stop_recording(file: UploadFile = File(...)):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
        temp_video.write(await file.read())
        video_path = temp_video.name
    
    cap = cv2.VideoCapture(video_path)
    frames = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()
    
    def analyze_video(frames):
        face_movements = []
        for frame in frames:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = mp_face_mesh.process(rgb_frame)
            if results.multi_face_landmarks:
                face_movements.append(results.multi_face_landmarks)
        return face_movements
    
    video_analysis = analyze_video(frames)
    
    audio = AudioSegment.from_file(video_path)
    audio_data = np.frombuffer(audio.raw_data, dtype=np.int16)
    audio_text = deepspeech_model.stt(audio_data)
    
    return {"video_analysis": video_analysis, "audio_transcription": audio_text}
